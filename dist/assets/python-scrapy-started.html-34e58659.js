import{_ as s,W as n,X as i,Y as e,Z as r,$ as p,a0 as t,D as d}from"./framework-f64bc974.js";const l={},o=t(`<h1 id="scrapy入门" tabindex="-1"><a class="header-anchor" href="#scrapy入门" aria-hidden="true">#</a> Scrapy入门</h1><h2 id="_1-简介" tabindex="-1"><a class="header-anchor" href="#_1-简介" aria-hidden="true">#</a> 1. 简介</h2><p>Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。</p><h3 id="_1-1-与requests-beautifulsoup-的区别" tabindex="-1"><a class="header-anchor" href="#_1-1-与requests-beautifulsoup-的区别" aria-hidden="true">#</a> 1.1 与Requests+BeautifulSoup 的区别</h3><ul><li><p>Requests+BeautifulSoup</p><p>适合快速实现的小项目</p></li><li><p>scrapy</p><p>大的工程化项目</p></li></ul><h2 id="_2-架构概览" tabindex="-1"><a class="header-anchor" href="#_2-架构概览" aria-hidden="true">#</a> 2. 架构概览</h2><figure><img src="https://zszblog.oss-cn-beijing.aliyuncs.com/zszblog/blogimage-master/img/image-20210310164531835.png" alt="image-20210310164531835" tabindex="0" loading="lazy"><figcaption>image-20210310164531835</figcaption></figure><h3 id="_2-1-scrapy-engine" tabindex="-1"><a class="header-anchor" href="#_2-1-scrapy-engine" aria-hidden="true">#</a> 2.1 Scrapy Engine</h3><blockquote><p>引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。</p></blockquote><p>此组件相当于爬虫的“大脑”，是整个爬虫的调度中心。</p><h3 id="_2-2-调度器-scheduler" tabindex="-1"><a class="header-anchor" href="#_2-2-调度器-scheduler" aria-hidden="true">#</a> 2.2 调度器（Scheduler）</h3><blockquote><p>调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。</p></blockquote><p>初始的爬取URL和后续在页面中获取的待爬取的URL将放入调度器中，等待爬取。同时调度器会自动去除重复的URL（如果特定的URL不需要去重也可以通过设置实现，如post请求的URL）</p><h3 id="_2-3-下载器-downloader" tabindex="-1"><a class="header-anchor" href="#_2-3-下载器-downloader" aria-hidden="true">#</a> 2.3 下载器（Downloader）</h3><p>下载器负责获取页面数据并提供给引擎，而后提供给spider。</p><h3 id="_2-4-spiders" tabindex="-1"><a class="header-anchor" href="#_2-4-spiders" aria-hidden="true">#</a> 2.4 Spiders</h3><p>Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。</p><h3 id="_2-5-item-pipeline" tabindex="-1"><a class="header-anchor" href="#_2-5-item-pipeline" aria-hidden="true">#</a> 2.5 Item Pipeline</h3><blockquote><p>Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。</p></blockquote><p>当页面被爬虫解析所需的数据存入Item后，将被发送到项目管道(Pipeline)，并经过几个特定的次序处理数据，最后存入本地文件或存入数据库。</p><h3 id="_2-6-下载器中间件-downloader-middlewares" tabindex="-1"><a class="header-anchor" href="#_2-6-下载器中间件-downloader-middlewares" aria-hidden="true">#</a> 2.6 下载器中间件(Downloader middlewares)</h3><blockquote><p>下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</p></blockquote><p>通过设置下载器中间件可以实现爬虫自动更换user-agent、IP等功能。</p><h3 id="_2-7-spider中间件-spider-middlewares" tabindex="-1"><a class="header-anchor" href="#_2-7-spider中间件-spider-middlewares" aria-hidden="true">#</a> 2.7 Spider中间件(Spider middlewares)</h3><blockquote><p>Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</p></blockquote><h3 id="_2-8-数据流-data-flow" tabindex="-1"><a class="header-anchor" href="#_2-8-数据流-data-flow" aria-hidden="true">#</a> 2.8 数据流(Data flow)</h3><ol><li>引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。</li><li>引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。</li><li>引擎向调度器请求下一个要爬取的URL。</li><li>调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。</li><li>一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。</li><li>引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。</li><li>Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。</li><li>引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。</li><li>(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。</li></ol><h2 id="_3-建立scrapy爬虫项目流程" tabindex="-1"><a class="header-anchor" href="#_3-建立scrapy爬虫项目流程" aria-hidden="true">#</a> 3. 建立Scrapy爬虫项目流程</h2><h3 id="_3-1-创建项目" tabindex="-1"><a class="header-anchor" href="#_3-1-创建项目" aria-hidden="true">#</a> 3.1 创建项目</h3><p>在开始爬取之前，首先要创建一个新的Scrapy项目。这里以爬取博客为例，进入你打算存储代码的目录中，运行下列命令:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>scrapy startproject scrapyspider
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>该命令将会创建包含下列内容的scrapyspider目录:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>scrapyspider/
    scrapy.cfg
    scrapyspider/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这些文件分别是:</p><ul><li>scrapy.cfg: 项目的配置文件。</li><li>scrapyspider/: 该项目的python模块。之后您将在此加入代码。</li><li>scrapyspider/items.py: 项目中的item文件。</li><li>scrapyspider/pipelines.py: 项目中的pipelines文件。</li><li>scrapyspider/settings.py: 项目的设置文件。</li><li>scrapyspider/spiders/: 放置spider代码的目录。</li></ul><h3 id="_3-2-编写第一个爬虫-spider" tabindex="-1"><a class="header-anchor" href="#_3-2-编写第一个爬虫-spider" aria-hidden="true">#</a> 3.2 编写第一个爬虫(Spider)</h3><blockquote><p>Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</p><p>其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。</p><p>为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性:</p><ul><li>name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</li><li>start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。</li><li>parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。</li></ul></blockquote><p>以下为我们的第一个Spider代码，保存在scrapyspider/spiders目录下的blog_spider.py文件中:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> Spider


<span class="token keyword">class</span> <span class="token class-name">BlogSpider</span><span class="token punctuation">(</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;woodenrobot&#39;</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;http://woodenrobot.me&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        titles <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;//a[@class=&quot;post-title-link&quot;]/text()&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> title <span class="token keyword">in</span> titles<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>title<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-3-启动爬虫" tabindex="-1"><a class="header-anchor" href="#_3-3-启动爬虫" aria-hidden="true">#</a> 3.3 启动爬虫</h3><p>打开终端进入项目所在路径(即:scrapyspider路径下)运行下列命令：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>scrapy crawl woodenrobot
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>启动爬虫后就可以看到打印出来当前页所有文章标题了。</p><h2 id="参考文章" tabindex="-1"><a class="header-anchor" href="#参考文章" aria-hidden="true">#</a> 参考文章</h2>`,44),c={href:"https://zhuanlan.zhihu.com/p/24669128",target:"_blank",rel:"noopener noreferrer"};function u(h,m){const a=d("ExternalLinkIcon");return n(),i("div",null,[o,e("p",null,[e("a",c,[r("Scrapy爬虫框架教程（一）-- Scrapy入门"),p(a)])])])}const v=s(l,[["render",u],["__file","python-scrapy-started.html.vue"]]);export{v as default};
